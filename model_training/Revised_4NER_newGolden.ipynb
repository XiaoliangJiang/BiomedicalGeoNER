{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPVQovOaG9Wm",
    "outputId": "b775e961-a54e-4005-ac0b-e66b40f48b2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score, classification_report, roc_curve, auc, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIfjSatAHfw8",
    "outputId": "8df2a157-7ad2-4142-f3f6-159fb7ff8dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# if running in google colab using this line to load data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04xcQPWYHf7X"
   },
   "outputs": [],
   "source": [
    "# Due to the size of this data is too large, please download it via this link before using\n",
    "# https://drive.google.com/file/d/14qRCzfshrPlUHfiqOpYZiLJx4tCFdVhj/view?usp=sharing\n",
    "df = pd.read_csv('/content/drive/My Drive/fulldata_p3_engi_self_cited_all_colab_revised_new.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkNN9ngMrTk9"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('/content/drive/My Drive/golden_engi_self_cited_all_colab_edited0515_h1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aaKPAOsraL8"
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'tagged_sentences': 'sentences'}, inplace=True)\n",
    "df1.rename(columns={'tagged_sentences': 'sentences'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JVglCCIIdyX",
    "outputId": "3680cd1e-fc7b-423c-a58a-d550dc553351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PMID', 'total_sentences', 'sentence_order', 'category', 'cand_length',\n",
       "       'cand_cap_num', 'cand_cap_ratio', 'cand_has_num', 'cand_has_punc',\n",
       "       'cand_capitalized', 'cand_words', 'cand_etal', 'comma_before',\n",
       "       'comma_after', 'parentheses_before', 'parentheses_after', 'year_before',\n",
       "       'year_after', 'etal_before', 'etal_after', 'cand_pos', 'mesh_rate_top1',\n",
       "       'mesh_rate_top2', 'mesh_rate_top3', 'mesh_rate_last1',\n",
       "       'mesh_rate_last2', 'mesh_rate_last3', 'MapAffil_dict_matched',\n",
       "       'cand_in_mesh', 'cand_in_affil', 'mapcity_in_mesh', 'mapcity_in_affil',\n",
       "       'mapstate_in_mesh', 'mapstate_in_affil', 'mapcountry_in_mesh',\n",
       "       'mapcountry_in_affil', 'stanstate_in_mesh', 'stancountry_in_mesh',\n",
       "       'stanregion_in_mesh', 'lastname_freq_log10', 'firstname_freq_log10',\n",
       "       'affiliation_freq_log10', 'title_freq_log10', 'abstract_freq_log10',\n",
       "       'cited_mesh_rate_top1', 'cited_mesh_rate_top2', 'cited_mesh_rate_top3',\n",
       "       'cited_mesh_rate_last1', 'cited_mesh_rate_last2',\n",
       "       'cited_mesh_rate_last3', 'cand_in_cited_mesh', 'cand_in_cited_affil',\n",
       "       'mapcity_in_cited_mesh', 'mapcity_in_cited_affil',\n",
       "       'mapstate_in_cited_mesh', 'mapstate_in_cited_affil',\n",
       "       'mapcountry_in_cited_mesh', 'mapcountry_in_cited_affil',\n",
       "       'stanstate_in_cited_mesh', 'stancountry_in_cited_mesh',\n",
       "       'stanregion_in_cited_mesh', 'sentences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAC_rk0XIr3e",
    "outputId": "7fc11711-52ac-46e6-9029-c8e29c28d7fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PMID', 'total_sentences', 'sentence_order', 'category', 'cand_length',\n",
       "       'cand_cap_num', 'cand_cap_ratio', 'cand_has_num', 'cand_has_punc',\n",
       "       'cand_capitalized', 'cand_words', 'cand_etal', 'comma_before',\n",
       "       'comma_after', 'parentheses_before', 'parentheses_after', 'year_before',\n",
       "       'year_after', 'etal_before', 'etal_after', 'cand_pos', 'mesh_rate_top1',\n",
       "       'mesh_rate_top2', 'mesh_rate_top3', 'mesh_rate_last1',\n",
       "       'mesh_rate_last2', 'mesh_rate_last3', 'MapAffil_dict_matched',\n",
       "       'cand_in_mesh', 'cand_in_affil', 'mapcity_in_mesh', 'mapcity_in_affil',\n",
       "       'mapstate_in_mesh', 'mapstate_in_affil', 'mapcountry_in_mesh',\n",
       "       'mapcountry_in_affil', 'stanstate_in_mesh', 'stancountry_in_mesh',\n",
       "       'stanregion_in_mesh', 'lastname_freq_log10', 'firstname_freq_log10',\n",
       "       'affiliation_freq_log10', 'title_freq_log10', 'abstract_freq_log10',\n",
       "       'cited_mesh_rate_top1', 'cited_mesh_rate_top2', 'cited_mesh_rate_top3',\n",
       "       'cited_mesh_rate_last1', 'cited_mesh_rate_last2',\n",
       "       'cited_mesh_rate_last3', 'cand_in_cited_mesh', 'cand_in_cited_affil',\n",
       "       'mapcity_in_cited_mesh', 'mapcity_in_cited_affil',\n",
       "       'mapstate_in_cited_mesh', 'mapstate_in_cited_affil',\n",
       "       'mapcountry_in_cited_mesh', 'mapcountry_in_cited_affil',\n",
       "       'stanstate_in_cited_mesh', 'stancountry_in_cited_mesh',\n",
       "       'stanregion_in_cited_mesh', 'sentences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPjTkEPXIMj3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_has_mesh_column(df):\n",
    "    # List of columns to check\n",
    "    columns_to_check = [\n",
    "        'mesh_rate_top1', 'mesh_rate_top2', 'mesh_rate_top3',\n",
    "        'mesh_rate_last1', 'mesh_rate_last2', 'mesh_rate_last3',\n",
    "        'cited_mesh_rate_top1', 'cited_mesh_rate_top2', 'cited_mesh_rate_top3',\n",
    "        'cited_mesh_rate_last1', 'cited_mesh_rate_last2', 'cited_mesh_rate_last3'\n",
    "    ]\n",
    "\n",
    "    # Check if all the specified columns are 0 for each row\n",
    "    df['has_mesh'] = (df[columns_to_check] != 0).any(axis=1).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_has_mesh_column(df)\n",
    "df1 = add_has_mesh_column(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCSafZ1xKAVY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_1 = df[df['category'] == 1]\n",
    "df_0 = df[df['category'] == 0]\n",
    "\n",
    "df_1_train = resample(df_1, n_samples=200000, random_state=42, replace=False)\n",
    "df_0_train = resample(df_0, n_samples=200000, random_state=42, replace=False)\n",
    "\n",
    "df_1_test = resample(df_1.drop(df_1_train.index), n_samples=50000, random_state=42, replace=False)\n",
    "df_0_test = resample(df_0.drop(df_0_train.index), n_samples=50000, random_state=42, replace=False)\n",
    "\n",
    "train_df = pd.concat([df_1_train, df_0_train])\n",
    "test_df = pd.concat([df_1_test, df_0_test])\n",
    "\n",
    "X_train = train_df.drop(['PMID','sentences',\"category\"], axis=1)\n",
    "y_train = train_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riVW51LQrfp3"
   },
   "outputs": [],
   "source": [
    "X_test1=df1.drop(['PMID','sentences',\"category\"], axis=1)\n",
    "y_test1=df1['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkepxkwhKQ1I",
    "outputId": "6cd46039-3e6d-4b29-8ccf-cce37765580f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cuda\n",
      "Your runtime has 67.4 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n",
      "This cross-sectional study evaluated baseline characteristics , clinical findings , management , and disposition of scorpion stung cases in 26 cities of 4 provinces in the southwest quarter of [ locB ] Iran [ locE ] , during one year.3008 cases of scorpion sting with mean age of 27.07 +- 16.58 years were studied ( 51.3 % female ) . \n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)\n",
    "\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:print('Not using a high-RAM runtime')\n",
    "else: print('You are using a high-RAM runtime!')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# define function\n",
    "def tokenize(texts):\n",
    "    max_len = 0\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "\n",
    "    idx = 2\n",
    "    for sent in texts:\n",
    "        try:\n",
    "            tokenized_sent = word_tokenize(sent) # tokenized sentence\n",
    "        except:\n",
    "            print(sent)\n",
    "        tokenized_texts.append(tokenized_sent) # list of tokenized sentence\n",
    "        \n",
    "        for token in tokenized_sent:\n",
    "      # try to give every token a idx and save as a dictionary\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        max_len = max(max_len, len(tokenized_sent))\n",
    "\n",
    "    return tokenized_texts, word2idx, max_len\n",
    "\n",
    "# Run the function\n",
    "all_text = train_df.sentences.to_list() + test_df.sentences.to_list()\n",
    "tokenized_texts, word2idx, max_len = tokenize(all_text)\n",
    "\n",
    "def encode(tokenized_texts, word2idx, max_len):\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "\n",
    "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
    "\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "    return np.array(input_ids)\n",
    "\n",
    "# Run the function\n",
    "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
    "\n",
    "key_list = list(word2idx.keys())\n",
    "val_list = list(word2idx.values())\n",
    "\n",
    "original_text=\"\"\n",
    "for i in input_ids[1]:\n",
    "    if i!=0:\n",
    "        position = val_list.index(i)\n",
    "        original_text+=(key_list[position])+\" \"\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpvgavNVKWsm",
    "outputId": "a1d54626-c9da-4e29-9954-f9cce2ce936d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS6d1fV4KXKB",
    "outputId": "13a7c3c3-21a6-42c9-f5ac-c95c14f8baba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28149128 -0.43328154 -0.27805266 ...  0.15989003  0.30415574\n",
      "  -0.48429793]\n",
      " [ 0.22714365  0.07440025 -0.10288147 ...  0.17574954 -0.12686293\n",
      "  -0.37174255]\n",
      " [ 0.10657177  0.41459867 -0.33068708 ...  0.34421924  0.42502624\n",
      "  -0.13311154]\n",
      " ...\n",
      " [ 0.02312319  0.5504053   0.1974358  ... -0.18385407 -0.23723741\n",
      "  -0.06174884]\n",
      " [ 0.35700214 -0.3478855  -0.09879243 ...  0.36169836  0.00952035\n",
      "  -0.35913733]\n",
      " [-0.11276294 -0.14919722  0.39412707 ... -0.02905211 -0.19184093\n",
      "  -0.00288671]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = list(train_df[\"sentences\"])\n",
    "\n",
    "model = SentenceTransformer('kamalkraj/BioSimCSE-BioLinkBERT-BASE')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "column_names = [f'feature{i+1}' for i in range(embeddings.shape[1])]\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nL52wmNorm3R",
    "outputId": "38592d80-50d1-40dc-f4da-edb8a3b26ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44321287  0.61106294  0.31195146 ...  0.02186558  0.3756872\n",
      "  -0.26630652]\n",
      " [-0.42530307  0.6032021   0.32422557 ...  0.02715944  0.36653665\n",
      "  -0.26881167]\n",
      " [-0.42296112  0.5817401   0.3331098  ...  0.03388963  0.34691784\n",
      "  -0.27075976]\n",
      " ...\n",
      " [-0.31099626  0.1418324   0.3566259  ... -0.01344258 -0.24386822\n",
      "  -0.23147172]\n",
      " [ 0.05902627 -0.33836463  0.05887907 ... -0.25943276 -0.46984458\n",
      "  -0.5125205 ]\n",
      " [ 0.13953839  0.15361442  0.11750158 ...  0.54712766  0.099871\n",
      "  -0.31657052]]\n"
     ]
    }
   ],
   "source": [
    "sentences_t = list(df1[\"sentences\"])\n",
    "\n",
    "language_model = SentenceTransformer('kamalkraj/BioSimCSE-BioLinkBERT-BASE')\n",
    "embeddings_t1 = language_model.encode(sentences_t)\n",
    "print(embeddings_t1)\n",
    "\n",
    "column_names = [f'feature{i+1}' for i in range(embeddings_t1.shape[1])]\n",
    "\n",
    "embeddings_t_df1 = pd.DataFrame(embeddings_t1, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WNF5cd2KcaY"
   },
   "outputs": [],
   "source": [
    "float_col=[\"cand_cap_ratio\",\"cand_pos\",\"mesh_rate_top1\",\"mesh_rate_top2\",\"mesh_rate_top3\",\"mesh_rate_last1\",\"mesh_rate_last2\",\"mesh_rate_last3\",\"title_freq_log10\",\"abstract_freq_log10\",\"cited_mesh_rate_top1\",\"cited_mesh_rate_top2\",\"cited_mesh_rate_top3\",\"cited_mesh_rate_last1\",\"cited_mesh_rate_last2\",\"cited_mesh_rate_last3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDaFkP9AKdvZ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for col in float_col:\n",
    "    X_train['sq_' + col] = X_train[col] ** 2\n",
    "    X_test1['sq_' + col] = X_test1[col] ** 2\n",
    "\n",
    "for col in float_col:\n",
    "    X_train['log_' + col] = np.where(X_train[col] == 0, 0, np.log(X_train[col]))\n",
    "    X_test1['log_' + col] = np.where(X_test1[col] == 0, 0, np.log(X_test1[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CstbMtguKglo",
    "outputId": "cc6b05bf-daf3-4f4f-f93f-5155203e0a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 92)\n",
      "(716, 92)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KrzNneXKitS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Ignore all warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Your full feature list should be included here. Make sure it's complete without an ellipsis.\n",
    "features_list=['cand_cap_numXMapAffil_dict_matched', 'total_sentencesXcand_capitalized', 'cand_wordsXtitle_freq_log10', 'cand_cap_ratioXlastname_freq_log10', 'cand_cap_ratioXaffiliation_freq_log10', 'cand_cap_numXmapcountry_in_cited_affil', 'cand_cap_ratioXfirstname_freq_log10', 'cand_cap_ratioXMapAffil_dict_matched', 'cand_wordsXaffiliation_freq_log10', 'cand_capitalizedXMapAffil_dict_matched', 'cand_wordsXMapAffil_dict_matched', 'cand_cap_numXcand_in_cited_affil', 'cand_lengthXcand_in_cited_affil', 'sentence_orderXcand_capitalized', 'affiliation_freq_log10Xmapcountry_in_cited_affil', 'cand_cap_numXcand_in_affil', 'abstract_freq_log10Xmapcountry_in_cited_affil', 'cand_cap_ratioXabstract_freq_log10', 'cand_lengthXmapstate_in_cited_affil', 'total_sentencesXcand_has_punc', 'cand_cap_ratioXmapcountry_in_cited_affil', 'cand_cap_ratioXtitle_freq_log10', 'cand_posXtitle_freq_log10', 'cand_cap_numXabstract_freq_log10', 'cand_wordsXlastname_freq_log10', 'cand_lengthXMapAffil_dict_matched', 'cand_in_cited_affilXmapcountry_in_cited_affil', 'cand_wordsXabstract_freq_log10', 'cand_lengthXcand_pos', 'MapAffil_dict_matchedXcand_in_cited_affil', 'total_sentencesXMapAffil_dict_matched', 'sentence_orderXcand_has_punc', 'cand_in_affilXmapcountry_in_affil', 'MapAffil_dict_matchedXaffiliation_freq_log10', 'affiliation_freq_log10Xcand_in_cited_affil', 'cand_capitalizedXmapcountry_in_cited_affil', 'cand_lengthXcited_mesh_rate_last3', 'MapAffil_dict_matchedXmapcountry_in_cited_affil', 'cand_wordsXmapcountry_in_cited_affil', 'lastname_freq_log10Xcand_in_cited_mesh', 'cand_posXMapAffil_dict_matched', 'cand_in_meshXfirstname_freq_log10', 'total_sentencesXcand_in_cited_affil', 'cand_capitalizedXcomma_after', 'firstname_freq_log10Xmapcountry_in_cited_affil', 'cand_lengthXcited_mesh_rate_last2', 'total_sentencesXcited_mesh_rate_last3', 'cand_lengthXmesh_rate_last1', 'cand_cap_numXmapcountry_in_affil', 'lastname_freq_log10Xstanregion_in_cited_mesh', 'cand_capitalizedXyear_after', 'cand_capitalizedXyear_before', 'cand_has_puncXcomma_after', 'cand_capitalizedXmapcountry_in_affil', 'cand_capitalizedXcomma_before', 'cand_wordsXmapcountry_in_affil', 'title_freq_log10Xabstract_freq_log10', 'parentheses_beforeXmapstate_in_cited_affil', 'sentence_orderXcand_in_cited_mesh', 'year_beforeXmapcountry_in_affil', 'sentence_orderXMapAffil_dict_matched', 'total_sentencesXcited_mesh_rate_last2', 'cand_capitalizedXparentheses_after', 'firstname_freq_log10Xmapstate_in_cited_affil', 'firstname_freq_log10Xcand_in_cited_affil', 'mesh_rate_top1Xtitle_freq_log10', 'total_sentencesXcand_has_num', 'cand_posXmapcountry_in_affil', 'total_sentencesXcand_pos', 'cand_in_meshXmapcountry_in_affil', 'total_sentencesXcited_mesh_rate_top3', 'comma_afterXmapcity_in_affil', 'etal_beforeXabstract_freq_log10', 'sentence_orderXcand_cap_num', 'MapAffil_dict_matchedXfirstname_freq_log10', 'cand_posXabstract_freq_log10', 'cand_cap_numXstancountry_in_mesh', 'cand_capitalizedXparentheses_before', 'title_freq_log10Xcand_in_cited_mesh', 'MapAffil_dict_matchedXcand_in_mesh', 'comma_afterXmapcountry_in_affil', 'affiliation_freq_log10Xabstract_freq_log10', 'cand_cap_numXstancountry_in_cited_mesh', 'mesh_rate_top2Xtitle_freq_log10', 'mesh_rate_top1Xmesh_rate_top2', 'cand_wordsXyear_before', 'cand_lengthXcited_mesh_rate_last1', 'cand_lengthXcand_in_cited_mesh', 'MapAffil_dict_matchedXcand_in_cited_mesh', 'cand_lengthXcand_in_affil', 'cand_lengthXmapstate_in_affil', 'cand_capitalizedXcand_in_affil', 'parentheses_beforeXMapAffil_dict_matched', 'cand_lengthXcand_in_mesh', 'year_beforeXMapAffil_dict_matched', 'cand_wordsXfirstname_freq_log10', 'mesh_rate_top1Xmesh_rate_top3', 'total_sentencesXstanstate_in_mesh', 'cand_lengthXmesh_rate_top3', 'total_sentencesXstanregion_in_mesh']\n",
    "\n",
    "# Dictionary to hold split feature pairs\n",
    "feature_pairs = {}\n",
    "\n",
    "# Split feature names and store in dictionary\n",
    "for feature in features_list:\n",
    "    base_features = feature.split('X')\n",
    "    feature_pairs[feature] = base_features\n",
    "\n",
    "# Assume X_train and X_test are already defined and include all the original features\n",
    "# Calculate the new product features for X_train and X_test\n",
    "for new_feature, (feature1, feature2) in feature_pairs.items():\n",
    "    X_train[new_feature] = X_train[feature1] * X_train[feature2]\n",
    "    # X_test2[new_feature] = X_test2[feature1] * X_test2[feature2]\n",
    "    X_test1[new_feature] = X_test1[feature1] * X_test1[feature2]\n",
    "\n",
    "# # Now X_train and X_test include all the new product features\n",
    "# print(X_train.head())  # Show some of the training data to verify the new features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EXsrFDwKlmL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train_new = pd.concat([X_train.reset_index(drop=True), embeddings_df.reset_index(drop=True)], axis=1)\n",
    "X_test_new1 = pd.concat([X_test1.reset_index(drop=True), embeddings_t_df1.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PVItfTLKni4",
    "outputId": "c28659cb-3eb4-418c-bf7c-baabd23a2abd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400000, 960), (716, 960), (716,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape,X_test_new1.shape,y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opPchXTlKp8C"
   },
   "outputs": [],
   "source": [
    "X_train_new[\"log_abstract_freq_log10\"] = X_train_new[\"log_abstract_freq_log10\"].fillna(0)\n",
    "X_train_new[\"log_title_freq_log10\"] = X_train_new[\"log_title_freq_log10\"].fillna(0)\n",
    "X_test_new1[\"log_abstract_freq_log10\"] = X_test_new1[\"log_abstract_freq_log10\"].fillna(0)\n",
    "X_test_new1[\"log_title_freq_log10\"] = X_test_new1[\"log_title_freq_log10\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4z6sKcNK0IJ"
   },
   "outputs": [],
   "source": [
    "X_test_new_spacy=X_test_new2[X_test_new2[\"from_tool\"]=='spaCy'].iloc[:,1:].reset_index(drop=True)\n",
    "X_test_new_nltk=X_test_new2[X_test_new2[\"from_tool\"]=='NLTK'].iloc[:,1:].reset_index(drop=True)\n",
    "X_test_new_stanza=X_test_new2[X_test_new2[\"from_tool\"]=='Stanza'].iloc[:,1:].reset_index(drop=True)\n",
    "X_test_new_flair=X_test_new2[X_test_new2[\"from_tool\"]=='FLAIR'].iloc[:,1:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uWwfZBLK0gS"
   },
   "outputs": [],
   "source": [
    "y_test_spacy=y_test2[X_test_new2[\"from_tool\"]=='spaCy'].reset_index(drop=True)\n",
    "y_test_nltk=y_test2[X_test_new2[\"from_tool\"]=='NLTK'].reset_index(drop=True)\n",
    "y_test_stanza=y_test2[X_test_new2[\"from_tool\"]=='Stanza'].reset_index(drop=True)\n",
    "y_test_flair=y_test2[X_test_new2[\"from_tool\"]=='FLAIR'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dNNy9WYkvgb",
    "outputId": "d5cedca8-a0a7-47bc-d0e0-8c8915933938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 50-50 gold, all features\n",
      "F1 Score: 0.9686609686609686\n",
      "Precision: 0.9855072463768116\n",
      "Recall: 0.9523809523809523\n",
      "\n",
      "Naive Bayes: 50-50 gold, all features\n",
      "F1 Score: 0.8501529051987767\n",
      "Precision: 0.936026936026936\n",
      "Recall: 0.7787114845938375\n",
      "\n",
      "XGBoost: 50-50 gold, all features\n",
      "F1 Score: 0.9518413597733709\n",
      "Precision: 0.9627507163323782\n",
      "Recall: 0.9411764705882353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns = list(X_train_new.columns[0:])\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new1[selected_columns])\n",
    "    print(f\"{name}: 50-50 gold, all features\")\n",
    "    print(f\"F1 Score: {f1_score(y_test1, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test1, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test1, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqdyDYMIs6EA",
    "outputId": "cef902a9-2d0a-4b1e-e1fd-225bbd96e87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 50-50 gold, no embedding\n",
      "F1 Score: 0.9053254437869822\n",
      "Precision: 0.9592476489028213\n",
      "Recall: 0.8571428571428571\n",
      "\n",
      "Naive Bayes: 50-50 gold, no embedding\n",
      "F1 Score: 0.8351309707241911\n",
      "Precision: 0.928082191780822\n",
      "Recall: 0.7591036414565826\n",
      "\n",
      "XGBoost: 50-50 gold, no embedding\n",
      "F1 Score: 0.9170305676855895\n",
      "Precision: 0.9545454545454546\n",
      "Recall: 0.8823529411764706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns = list(X_train_new.columns[:192])\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new1[selected_columns])\n",
    "    print(f\"{name}: 50-50 gold, no embedding\")\n",
    "    print(f\"F1 Score: {f1_score(y_test1, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test1, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test1, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3svkncwtRrG",
    "outputId": "1a514264-a95c-4c01-eacf-033fd1d6f2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 50-50 gold, only embedding\n",
      "F1 Score: 0.9308005427408413\n",
      "Precision: 0.9026315789473685\n",
      "Recall: 0.9607843137254902\n",
      "\n",
      "Naive Bayes: 50-50 gold, only embedding\n",
      "F1 Score: 0.9086161879895561\n",
      "Precision: 0.8508557457212714\n",
      "Recall: 0.9747899159663865\n",
      "\n",
      "XGBoost: 50-50 gold, only embedding\n",
      "F1 Score: 0.9322493224932249\n",
      "Precision: 0.9028871391076115\n",
      "Recall: 0.9635854341736695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns = list(X_train_new.columns[192:])\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new1[selected_columns])\n",
    "    print(f\"{name}: 50-50 gold, only embedding\")\n",
    "    print(f\"F1 Score: {f1_score(y_test1, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test1, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test1, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfSg_Jh7tU7h",
    "outputId": "95935566-3db6-4b47-81d2-74af9dd5d78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.2174\n",
      "Epoch [2/7], Loss: 0.1126\n",
      "Epoch [3/7], Loss: 0.1808\n",
      "Epoch [4/7], Loss: 0.0793\n",
      "Epoch [5/7], Loss: 0.1955\n",
      "Epoch [6/7], Loss: 0.0399\n",
      "Epoch [7/7], Loss: 0.0515\n",
      "only embedding\n",
      "Precision: 0.9178\n",
      "Recall: 0.9384\n",
      "F1 Score: 0.9280\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def create_dataloaders_from_frames(X_train, X_test, y_train, y_test, batch_size=50):\n",
    "    train_features = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    test_features = torch.tensor(X_test.values, dtype=torch.float)\n",
    "\n",
    "    train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    train_data = TensorDataset(train_features, train_labels)\n",
    "    test_data = TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new.iloc[:,192:], X_test_new1.iloc[:,192:], y_train, y_test1)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(768, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 16)\n",
    "        self.layer7 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.sigmoid(self.layer7(x))\n",
    "        return x\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "model = NeuralNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 7 \n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.float()  \n",
    "        labels = labels.float().unsqueeze(1)  \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()  \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"only embedding\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AkvIf-_tm-A",
    "outputId": "2e6be585-1005-4a62-8df9-dde8cd7bf600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.0527\n",
      "Epoch [2/7], Loss: 0.0437\n",
      "Epoch [3/7], Loss: 0.0535\n",
      "Epoch [4/7], Loss: 0.1128\n",
      "Epoch [5/7], Loss: 0.0910\n",
      "Epoch [6/7], Loss: 0.0702\n",
      "Epoch [7/7], Loss: 0.0457\n",
      "50-50 gold all embedding\n",
      "Precision: 0.9580\n",
      "Recall: 0.8936\n",
      "F1 Score: 0.9246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def create_dataloaders_from_frames(X_train, X_test, y_train, y_test, batch_size=50):\n",
    "    train_features = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    test_features = torch.tensor(X_test.values, dtype=torch.float)\n",
    "\n",
    "    train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    train_data = TensorDataset(train_features, train_labels)\n",
    "    test_data = TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new, X_test_new1, y_train, y_test1)\n",
    "\n",
    "# .iloc[:,59:827] only embeddings\n",
    "# train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new.iloc[:,59:827], X_test_new.iloc[:,59:827], y_train, y_test)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(960, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 16)\n",
    "        self.layer7 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.sigmoid(self.layer7(x))\n",
    "        return x\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model = NeuralNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 7 #5\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.float()  \n",
    "        labels = labels.float().unsqueeze(1)  \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()  \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"50-50 gold all embedding\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odX-CGCzlEO6"
   },
   "outputs": [],
   "source": [
    "selected_columns = list(X_train_new.columns[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K53GMd_mptJ7"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_gold[selected_columns])\n",
    "    print(f\"{name}: gold, all features\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_gold, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "mWTBf920O0pu",
    "outputId": "de169626-c95e-4cdf-d202-d4b6d8a7f065"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new, y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new, y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ag_1N14lO2q"
   },
   "outputs": [],
   "source": [
    "selected_columns = list(X_train_new.columns[0:])\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_gold[selected_columns])\n",
    "    print(f\"{name}: gold, all features\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_gold, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDrDR3N0OQPk"
   },
   "outputs": [],
   "source": [
    "selected_columns = list(X_train_new.columns[:191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyXYu5gFNim7",
    "outputId": "b41c7ef9-5159-4876-c8a9-c1d3604160a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: No Embedding\n",
      "F1 Score: 0.9050527484730706\n",
      "Precision: 0.8725910064239829\n",
      "Recall: 0.9400230680507498\n",
      "\n",
      "Naive Bayes: No Embedding\n",
      "F1 Score: 0.8108108108108107\n",
      "Precision: 0.7769556025369979\n",
      "Recall: 0.8477508650519031\n",
      "\n",
      "XGBoost: No Embedding\n",
      "F1 Score: 0.9211850195640022\n",
      "Precision: 0.8937093275488069\n",
      "Recall: 0.9504036908881199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_gold[selected_columns])\n",
    "    print(f\"{name}: No Embedding\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_gold, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvjZ3MamN1e8",
    "outputId": "21630d4b-8717-46e6-dc39-e81f0e9528ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Only Embedding\n",
      "F1 Score: 0.9225700164744646\n",
      "Precision: 0.8805031446540881\n",
      "Recall: 0.9688581314878892\n",
      "\n",
      "Naive Bayes: Only Embedding\n",
      "F1 Score: 0.9165302782324057\n",
      "Precision: 0.8695652173913043\n",
      "Recall: 0.9688581314878892\n",
      "\n",
      "XGBoost: Only Embedding\n",
      "F1 Score: 0.927123287671233\n",
      "Precision: 0.8830897703549061\n",
      "Recall: 0.9757785467128027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns = list(X_train_new.columns[191:])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new[selected_columns], y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_gold[selected_columns])\n",
    "    print(f\"{name}: Only Embedding\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_gold, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_gold, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ax2Ms26oSfY4",
    "outputId": "7dd455a0-2acf-4015-9e1b-c7e682a1362d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400000, 769), (1604, 769))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.iloc[:,191:].shape,X_test_new_gold.iloc[:,191:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tu7exq-HPs8K",
    "outputId": "4321ffc6-46b4-433e-a7bc-a56cb4cec1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.1835\n",
      "Epoch [2/7], Loss: 0.1617\n",
      "Epoch [3/7], Loss: 0.3140\n",
      "Epoch [4/7], Loss: 0.1983\n",
      "Epoch [5/7], Loss: 0.0875\n",
      "Epoch [6/7], Loss: 0.1332\n",
      "Epoch [7/7], Loss: 0.1518\n",
      "only embedding\n",
      "Precision: 0.8797\n",
      "Recall: 0.9619\n",
      "F1 Score: 0.9190\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def create_dataloaders_from_frames(X_train, X_test, y_train, y_test, batch_size=50):\n",
    "    train_features = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    test_features = torch.tensor(X_test.values, dtype=torch.float)\n",
    "\n",
    "    train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    train_data = TensorDataset(train_features, train_labels)\n",
    "    test_data = TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new.iloc[:,192:], X_test_new_gold.iloc[:,192:], y_train, y_test_gold)\n",
    "\n",
    "# .iloc[:,59:827] only embeddings\n",
    "# train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new.iloc[:,59:827], X_test_new.iloc[:,59:827], y_train, y_test)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(768, 512)\n",
    "        # self.layer1 = nn.Linear(959, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 16)\n",
    "        self.layer7 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.sigmoid(self.layer7(x))\n",
    "        return x\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model = NeuralNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 7 #5\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.float()  \n",
    "        labels = labels.float().unsqueeze(1)  \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()  \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"only embedding\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYHqbenYMOW_",
    "outputId": "0d337eb5-58c9-4bc8-e4f7-abc3cfd395dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Stanza\n",
      "F1 Score: 0.9251247920133111\n",
      "Precision: 0.8714733542319749\n",
      "Recall: 0.9858156028368794\n",
      "\n",
      "Naive Bayes: Stanza\n",
      "F1 Score: 0.8829787234042553\n",
      "Precision: 0.8829787234042553\n",
      "Recall: 0.8829787234042553\n",
      "\n",
      "XGBoost: Stanza\n",
      "F1 Score: 0.9233333333333335\n",
      "Precision: 0.8710691823899371\n",
      "Recall: 0.9822695035460993\n",
      "\n",
      "Logistic Regression: spaCy\n",
      "F1 Score: 0.9400749063670412\n",
      "Precision: 0.8932384341637011\n",
      "Recall: 0.9920948616600791\n",
      "\n",
      "Naive Bayes: spaCy\n",
      "F1 Score: 0.7232\n",
      "Precision: 0.6075268817204301\n",
      "Recall: 0.8932806324110671\n",
      "\n",
      "XGBoost: spaCy\n",
      "F1 Score: 0.9211009174311927\n",
      "Precision: 0.8595890410958904\n",
      "Recall: 0.9920948616600791\n",
      "\n",
      "Logistic Regression: FLAIR\n",
      "F1 Score: 0.9451851851851852\n",
      "Precision: 0.9300291545189504\n",
      "Recall: 0.9608433734939759\n",
      "\n",
      "Naive Bayes: FLAIR\n",
      "F1 Score: 0.8673946957878315\n",
      "Precision: 0.8996763754045307\n",
      "Recall: 0.8373493975903614\n",
      "\n",
      "XGBoost: FLAIR\n",
      "F1 Score: 0.9394387001477106\n",
      "Precision: 0.9217391304347826\n",
      "Recall: 0.9578313253012049\n",
      "\n",
      "Logistic Regression: NLTK\n",
      "F1 Score: 0.6866141732283464\n",
      "Precision: 0.5396039603960396\n",
      "Recall: 0.9437229437229437\n",
      "\n",
      "Naive Bayes: NLTK\n",
      "F1 Score: 0.5202156334231806\n",
      "Precision: 0.3776908023483366\n",
      "Recall: 0.8354978354978355\n",
      "\n",
      "XGBoost: NLTK\n",
      "F1 Score: 0.6875\n",
      "Precision: 0.5378973105134475\n",
      "Recall: 0.9523809523809523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_new, y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_new, y_train)\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train_new, y_train)\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Naive Bayes': nb, 'XGBoost': xgb}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_stanza)\n",
    "    print(f\"{name}: Stanza\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_stanza, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_stanza, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_stanza, y_pred)}\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_spacy)\n",
    "    print(f\"{name}: spaCy\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_spacy, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_spacy, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_spacy, y_pred)}\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_flair)\n",
    "    print(f\"{name}: FLAIR\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_flair, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_flair, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_flair, y_pred)}\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_new_nltk)\n",
    "    print(f\"{name}: NLTK\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_nltk, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test_nltk, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_test_nltk, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u9IZEyAM53V",
    "outputId": "3a786e93-0e65-450d-d26f-8c5b2e3b8d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.1561\n",
      "Epoch [2/7], Loss: 0.0568\n",
      "Epoch [3/7], Loss: 0.0588\n",
      "Epoch [4/7], Loss: 0.0585\n",
      "Epoch [5/7], Loss: 0.1172\n",
      "Epoch [6/7], Loss: 0.1022\n",
      "Epoch [7/7], Loss: 0.0180\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def create_dataloaders_from_frames(X_train, X_test, y_train, y_test, batch_size=50):\n",
    "    train_features = torch.tensor(X_train.values, dtype=torch.float)\n",
    "    test_features = torch.tensor(X_test.values, dtype=torch.float)\n",
    "\n",
    "    train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "    train_data = TensorDataset(train_features, train_labels)\n",
    "    test_data = TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new, X_test_new_stanza, y_train, y_test_stanza)\n",
    "# X_test_new_flair, X_test_new_spacy, X_test_new_nltk, X_test_new_stanza\n",
    "\n",
    "# .iloc[:,59:827] only embeddings\n",
    "# train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new.iloc[:,59:827], X_test_new.iloc[:,59:827], y_train, y_test)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # self.layer1 = nn.Linear(768, 512)\n",
    "        self.layer1 = nn.Linear(960, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 16)\n",
    "        self.layer7 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.sigmoid(self.layer7(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 7 #5\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.float()  \n",
    "        labels = labels.float().unsqueeze(1)  \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ne9LZuxNQM8m",
    "outputId": "752ef2d0-5bcd-42dd-a0b4-3c7925de375f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza\n",
      "Precision: 0.8576\n",
      "Recall: 0.9823\n",
      "F1 Score: 0.9157\n",
      "spacy\n",
      "Precision: 0.8591\n",
      "Recall: 0.9881\n",
      "F1 Score: 0.9191\n",
      "flair\n",
      "Precision: 0.9162\n",
      "Recall: 0.9548\n",
      "F1 Score: 0.9351\n",
      "nltk\n",
      "Precision: 0.5143\n",
      "Recall: 0.9351\n",
      "F1 Score: 0.6636\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new, X_test_new_stanza, y_train, y_test_stanza)\n",
    "\n",
    "model.eval() \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"stanza\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new, X_test_new_spacy, y_train, y_test_spacy)\n",
    "\n",
    "model.eval()  \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"spacy\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new, X_test_new_flair, y_train, y_test_flair)\n",
    "\n",
    "model.eval()  \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"flair\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders_from_frames(X_train_new, X_test_new_nltk, y_train, y_test_nltk)\n",
    "\n",
    "model.eval()  \n",
    "predictions = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.round().squeeze().long()  \n",
    "        predictions.extend(predicted.tolist())\n",
    "        targets.extend(labels.tolist())\n",
    "\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "print(\"nltk\")\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv-ZlRkmQnNT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
