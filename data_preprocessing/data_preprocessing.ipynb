{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8988dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece236aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of generating 12m-13m data\n",
    "skip_rows = 12000001 \n",
    "n_rows_to_read = 1000000 \n",
    "\n",
    "start_time = time.time()\n",
    "column_names=['PMID', 'title','abstract']\n",
    "# In total 18.8m rows，can be downloaded from:\n",
    "# http://abel.lis.illinois.edu/data/abstracts2018.tsv.gz\n",
    "abs_df = pd.read_csv('abstracts2018.tsv.gz',compression='gzip', sep='\\t',on_bad_lines='warn',nrows=n_rows_to_read, skiprows=skip_rows,names=column_names)\n",
    "end_time = time.time()\n",
    "print(f\"processing time：{end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "count = 0\n",
    "temp,start_time=time.time(),time.time()\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in abs_df.iterrows():\n",
    "    \n",
    "    if count % 10000 == 0:\n",
    "        cost = time.time() - temp\n",
    "        now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Current time is: {now}. {count//10000}% done. {count} rows processed, time taken: {round(cost)} seconds. Size of data is {round(sys.getsizeof(results)/1024/1024,2)}MB\")\n",
    "        temp = time.time()\n",
    "    \n",
    "    if isinstance(row['abstract'], str):\n",
    "        doc = nlp(row['abstract'])\n",
    "        sentences = list(doc.sents)  \n",
    "        total_sentences = len(sentences)\n",
    "        \n",
    "        for i, sent in enumerate(sentences):\n",
    "            gpe_entities = [ent.text for ent in sent.ents if ent.label_ == \"GPE\"]\n",
    "            if gpe_entities:\n",
    "                results.append({\n",
    "                    'PMID': row['PMID'],\n",
    "                    'total_sentences': total_sentences,\n",
    "                    'sentence_order': i + 1,\n",
    "                    'sentence': sent.text,\n",
    "                    'candidates': gpe_entities\n",
    "                })\n",
    "    else:\n",
    "        print(f\"Non-string value encountered at index {count}: {row['abstract']}\")\n",
    "    count += 1\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processing completed in {round((end_time - start_time)/3600,2)} hours.\")\n",
    "result_df.to_excel('new_candidates_12m_13m.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = ''\n",
    "\n",
    "# combine all generated data\n",
    "# replace the next line by your file names\n",
    "file_order = ['new_candidates_0-1m.xlsx', 'new_candidates_1m-2m5.xlsx', 'new_candidates_2m5-4m.xlsx', 'new_candidates_4m-6m.xlsx', 'new_candidates_6m-7m5.xlsx', 'new_candidates_7m5-9m.xlsx', 'new_candidates_9m-10m.xlsx', 'new_candidates_10m-11m.xlsx', 'new_candidates_11m-12m.xlsx', 'new_candidates_12m-13m.xlsx', 'new_candidates_13m_14m.xlsx', 'new_candidates_14m-15m5_1.xlsx', 'new_candidates_14m-15m5_2.xlsx', 'new_candidates_15m5-17m_1.xlsx', 'new_candidates_15m5-17m_2.xlsx', 'new_candidates_17m-18m.xlsx', 'new_candidates_18m-19m.xlsx']\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for filename in file_order:\n",
    "    print(filename)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_excel(file_path)\n",
    "    all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "if \"Unnamed: 0\" in all_data.columns:\n",
    "    all_data.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('all_candidates.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.read_csv('all_candidates.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a03aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014-2018 after 7063577 PMID=24267737 \n",
    "all_data=all_data[7063577:].reset_index(drop=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa90760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "stanza.download('en')  \n",
    "nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_stanza(sentence):\n",
    "    doc = nlp_stanza(sentence)\n",
    "    return [ent.text for sent in doc.sentences for ent in sent.ents if ent.type == 'GPE']\n",
    "\n",
    "def extract_entities_nltk(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    chunks = nltk.ne_chunk(tags)\n",
    "    return [' '.join(leaf[0] for leaf in c.leaves()) for c in chunks if hasattr(c, 'label') and c.label() == 'GPE']\n",
    "\n",
    "def extract_entities_transformers(sentence):\n",
    "    entities = nlp_transformers(sentence)\n",
    "    return [entity['word'] for entity in entities if entity['entity'] == 'I-LOC' or entity['entity'] == 'B-LOC']\n",
    "\n",
    "def extract_entities_flair(sentence):\n",
    "    sentence = Sentence(sentence)\n",
    "    tagger.predict(sentence)\n",
    "    return [entity.text for entity in sentence.get_spans('ner') if entity.tag == 'LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "test_data = pd.DataFrame(index=all_data.index)\n",
    "\n",
    "test_data = all_data[2500000:3500000].copy()\n",
    "test_data = test_data.dropna(subset=['sentence'])\n",
    "\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "test_data[test_data[\"sentence\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5423dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['stanza_candidates']:\n",
    "    test_data[column] = None  #\n",
    "\n",
    "def process_data(source_data, target_data):\n",
    "    total_rows = len(source_data)\n",
    "\n",
    "    for index, row in tqdm(source_data.iterrows(), total=total_rows, desc=\"Processing\"):\n",
    "        sentence = row['sentence']\n",
    "        target_data.at[index, 'stanza_candidates'] = extract_entities_stanza(sentence)\n",
    "\n",
    "    return target_data\n",
    "\n",
    "test_data = process_data(test_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9dff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_longest_common_substring(str1, str2):\n",
    "    \"\"\" Find the longest common substring between two strings. \"\"\"\n",
    "    sequence_matcher = SequenceMatcher(None, str1, str2)\n",
    "    match = sequence_matcher.find_longest_match(0, len(str1), 0, len(str2))\n",
    "    if match.size == 0:\n",
    "        return None\n",
    "    return str1[match.a: match.a + match.size]\n",
    "\n",
    "def common_substring_in_lists(list1, list2):\n",
    "    \"\"\" Return common substrings between two lists of strings. \"\"\"\n",
    "    common_substrings = set()\n",
    "    for str1 in list1:\n",
    "        for str2 in list2:\n",
    "            substring = find_longest_common_substring(str1, str2)\n",
    "            if substring:\n",
    "                common_substrings.add(substring)\n",
    "    return list(common_substrings)\n",
    "\n",
    "def process_row(row):\n",
    "    candidates = ast.literal_eval(row['candidates']) if isinstance(row['candidates'], str) else row['candidates']\n",
    "    stanza_candidates = ast.literal_eval(row['stanza_candidates']) if isinstance(row['stanza_candidates'], str) else row['stanza_candidates']\n",
    "\n",
    "    # Finding common entities between candidates and stanza_candidates (bagging_result1)\n",
    "    bagging_result1 = common_substring_in_lists(candidates, stanza_candidates)\n",
    "\n",
    "    return pd.Series({'bagging_result1': bagging_result1})\n",
    "\n",
    "# Assuming test_data is a DataFrame with the mentioned columns.\n",
    "# Update the DataFrame by applying the function to each row\n",
    "with tqdm(total=len(test_data), desc=\"Processing rows\") as pbar:\n",
    "    results = test_data.apply(lambda row: process_row(row), axis=1)\n",
    "    test_data = test_data.join(results)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_results(results):\n",
    "    filtered_results = [result for result in results if len(result) > 1 and any(c.isupper() for c in result)]\n",
    "    return filtered_results\n",
    "\n",
    "test_data['bagging_result_filter'] = test_data['bagging_result1'].apply(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91086a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  #\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "def safe_eval_literal(string):\n",
    "    try:\n",
    "        return ast.literal_eval(string)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  \n",
    "\n",
    "def split_rows(row):\n",
    "    if type(row['bagging_result_filter'])==list:\n",
    "        bagging_result_filter = row['bagging_result_filter']\n",
    "    else:\n",
    "        bagging_result_filter = safe_eval_literal(row['bagging_result_filter'])  \n",
    "        \n",
    "    if type(row['candidates'])==list:\n",
    "        candidates = row['candidates']\n",
    "    else:\n",
    "        candidates = safe_eval_literal(row['candidates'])\n",
    "        \n",
    "  \n",
    "    category = 1 if bagging_result_filter else 0\n",
    "    items_to_split = bagging_result_filter if bagging_result_filter else candidates\n",
    "    \n",
    "    new_rows = []\n",
    "    if items_to_split:\n",
    "        for item in items_to_split:\n",
    "            new_row = row.to_dict()\n",
    "            new_row['category'] = category\n",
    "            new_row['bagging_result_filter'] = [item] if category else new_row['bagging_result_filter']\n",
    "            new_row['candidates'] = [item] if not category else new_row['candidates']\n",
    "            new_rows.append(new_row)\n",
    "    else:\n",
    "        new_row = row.to_dict()\n",
    "        new_row['category'] = category\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    return new_rows\n",
    "\n",
    "expanded_data = []\n",
    "for _, row in tqdm(test_data.iterrows(), total=test_data.shape[0], desc=\"Processing Rows\"):\n",
    "    expanded_data.extend(split_rows(row))\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "expanded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "expanded_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867308de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detected_candidate(row):\n",
    "    candidate = row['bagging_result_filter'] if row['category'] == 1 else row['candidates']\n",
    "    return ' '.join(candidate) if isinstance(candidate, list) else candidate\n",
    "\n",
    "expanded_df['detected_candidate'] = expanded_df.apply(create_detected_candidate, axis=1)\n",
    "\n",
    "def tag_sentence(row):\n",
    "    sentence = row['sentence']\n",
    "    candidate = row['detected_candidate']\n",
    "    tagged_sentence = sentence.replace(candidate, f\"[locB] {candidate} [locE]\")\n",
    "    return tagged_sentence\n",
    "\n",
    "expanded_df['tagged_sentences'] = expanded_df.apply(tag_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.to_csv('test_bagging_res_tagged.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
